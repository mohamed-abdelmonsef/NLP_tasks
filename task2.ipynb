{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy init config base_config.cfg --lang en --pipeline ner"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aj5iwVk3rNPy",
        "outputId": "b0226c52-d4ab-4a3f-c045-32cd8aaed1c3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;3m⚠ To generate a more effective transformer-based config (GPU-only),\n",
            "install the spacy-transformers package and re-run this command. The config\n",
            "generated now does not use transformers.\u001b[0m\n",
            "\u001b[38;5;4mℹ Generated config template specific for your use case\u001b[0m\n",
            "- Language: en\n",
            "- Pipeline: ner\n",
            "- Optimize for: efficiency\n",
            "- Hardware: CPU\n",
            "- Transformer: None\n",
            "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
            "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
            "base_config.cfg\n",
            "You can now add your data and train your pipeline:\n",
            "python -m spacy train base_config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy init fill-config base_config.cfg config.cfg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xrh2M4j2ucy-",
        "outputId": "a3c5882f-19ad-4c87-81ab-8954ac0bfe13"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;3m⚠ Nothing to auto-fill: base config is already complete\u001b[0m\n",
            "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
            "config.cfg\n",
            "You can now add your data and train your pipeline:\n",
            "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from datasets import load_dataset\n",
        "from spacy.tokens import DocBin\n",
        "from spacy.training import Example\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_dataset(\"conll2003\")\n",
        "\n",
        "# Load blank English pipeline\n",
        "nlp = spacy.blank(\"en\")\n",
        "\n",
        "def convert_to_spacy(data, output_path):\n",
        "    db = DocBin()\n",
        "    for item in data:\n",
        "        tokens = item[\"tokens\"]\n",
        "        ner_tags = item[\"ner_tags\"]\n",
        "        tags = [dataset[\"train\"].features[\"ner_tags\"].feature.int2str(tag) for tag in ner_tags]\n",
        "\n",
        "        doc = nlp.make_doc(\" \".join(tokens))\n",
        "        ents = []\n",
        "        start = 0\n",
        "\n",
        "        for token, tag in zip(tokens, tags):\n",
        "            token_start = doc.text.find(token, start)\n",
        "            token_end = token_start + len(token)\n",
        "            if tag.startswith(\"B-\"):\n",
        "                ent_start = token_start\n",
        "                ent_end = token_end\n",
        "                ent_label = tag[2:]\n",
        "            elif tag.startswith(\"I-\") and 'ent_start' in locals():\n",
        "                ent_end = token_end\n",
        "            else:\n",
        "                if tag != \"O\" and 'ent_start' in locals():\n",
        "                    span = doc.char_span(ent_start, ent_end, label=ent_label)\n",
        "                    if span:\n",
        "                        ents.append(span)\n",
        "                    del ent_start, ent_end, ent_label\n",
        "            start = token_end\n",
        "\n",
        "        # Final check to catch last entity\n",
        "        if 'ent_start' in locals():\n",
        "            span = doc.char_span(ent_start, ent_end, label=ent_label)\n",
        "            if span:\n",
        "                ents.append(span)\n",
        "            del ent_start, ent_end, ent_label\n",
        "\n",
        "        doc.ents = ents\n",
        "        db.add(doc)\n",
        "\n",
        "    db.to_disk(output_path)\n",
        "\n",
        "# Convert train/dev/test\n",
        "convert_to_spacy(dataset[\"train\"], \"train.spacy\")\n",
        "convert_to_spacy(dataset[\"validation\"], \"dev.spacy\")\n",
        "convert_to_spacy(dataset[\"test\"], \"test.spacy\")\n"
      ],
      "metadata": {
        "id": "rcRwlI4Eu9us"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy train config.cfg \\\n",
        "  --output ./output \\\n",
        "  --paths.train ./train.spacy \\\n",
        "  --paths.dev ./dev.spacy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXGLIjpFwiBf",
        "outputId": "a98aa013-7f2e-4e6c-e4d8-6a88847f5f83"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Created output directory: output\u001b[0m\n",
            "\u001b[38;5;4mℹ Saving to output directory: output\u001b[0m\n",
            "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
            "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  ------------  --------  ------  ------  ------  ------\n",
            "  0       0          0.00     41.28    0.00    0.00    0.00    0.00\n",
            "  0     200         77.78   2173.66   27.33   62.30   17.50    0.27\n",
            "  0     400        106.83   1526.14   52.30   68.17   42.42    0.52\n",
            "  0     600        153.42   1661.20   58.55   68.14   51.32    0.59\n",
            "  0     800        238.54   1813.44   60.53   76.37   50.13    0.61\n",
            "  0    1000        287.46   2125.64   64.73   68.36   61.46    0.65\n",
            "  1    1200        349.57   2309.44   67.29   74.91   61.07    0.67\n",
            "  1    1400        502.90   2330.11   66.77   66.74   66.79    0.67\n",
            "  1    1600        510.11   2685.65   69.99   77.67   63.69    0.70\n",
            "  2    1800        607.33   2770.52   71.17   73.28   69.17    0.71\n",
            "  2    2000        800.67   3046.89   71.66   75.27   68.37    0.72\n",
            "  3    2200        956.67   2943.69   71.96   78.47   66.45    0.72\n",
            "  4    2400       1057.09   2889.03   70.74   68.87   72.71    0.71\n",
            "  5    2600       1116.96   2515.09   71.87   73.65   70.17    0.72\n",
            "  6    2800        989.79   2127.55   71.80   70.09   73.59    0.72\n",
            "  7    3000        993.97   1831.94   71.06   69.99   72.17    0.71\n",
            "  8    3200       1004.15   1631.17   71.90   73.50   70.36    0.72\n",
            "  9    3400       1675.04   1484.72   71.63   74.91   68.64    0.72\n",
            "  9    3600       1025.39   1324.04   71.58   71.37   71.79    0.72\n",
            " 10    3800        991.03   1213.92   71.02   76.74   66.10    0.71\n",
            "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
            "output/model-last\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('./output/model-best')"
      ],
      "metadata": {
        "id": "QTf-oVMV5tAW"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy evaluate ./output/model-best ./test.spacy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDdNPTpN5tob",
        "outputId": "02cb0f5b-8099-40d9-f206-70cf8f3d6abc"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
            "\u001b[1m\n",
            "================================== Results ==================================\u001b[0m\n",
            "\n",
            "TOK     100.00\n",
            "NER P   75.99 \n",
            "NER R   64.55 \n",
            "NER F   69.81 \n",
            "SPEED   17171 \n",
            "\n",
            "\u001b[1m\n",
            "=============================== NER (per type) ===============================\u001b[0m\n",
            "\n",
            "           P       R       F\n",
            "PER    61.80   53.03   57.08\n",
            "ORG    76.05   66.70   71.07\n",
            "LOC    84.33   75.45   79.64\n",
            "MISC   68.30   43.47   53.12\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"trump usa Germany hi John .\"\n",
        "doc = nlp(text)\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hk1yjuID50nE",
        "outputId": "d553ccd0-598c-490c-876f-ac2a0124e7a3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "John PER\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2g9sy8H16FaQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}